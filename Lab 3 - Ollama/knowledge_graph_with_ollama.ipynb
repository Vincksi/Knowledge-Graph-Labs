{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5576939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import urllib.request\n",
    "import time\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0056dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "from langchain_experimental.graph_transformers.diffbot import DiffbotGraphTransformer\n",
    "from langchain_core.documents import Document\n",
    "from langchain_neo4j import Neo4jGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39170c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for Diffbot API Key\n",
    "os.environ[\"DIFFBOT_API_KEY\"] = \"da94fef74b2133457fec1bfd3293855c\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b819b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Split Wikipedia Data\n",
    "def load_and_split_wikipedia_article(query: str) -> List[Document]:\n",
    "    \"\"\"Load a Wikipedia article and split it into chunks.\"\"\"\n",
    "\n",
    "    print(\"Loading Wikipedia article...\")\n",
    "    raw_documents = WikipediaLoader(query=query).load()\n",
    "    \n",
    "    print(\"Splitting documents into chunks...\")\n",
    "    text_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=24)\n",
    "    documents = text_splitter.split_documents(raw_documents)\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65a688eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Wikipedia article...\n",
      "Splitting documents into chunks...\n",
      "Loaded and split 50 document chunks from Wikipedia article on Yann LeCun.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "query = \"Yann LeCun\"\n",
    "wiki_document = load_and_split_wikipedia_article(query)\n",
    "\n",
    "print(f\"Loaded and split {len(wiki_document)} document chunks from Wikipedia article on {query}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd84d3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Graph Data with Diffbot\n",
    "def extract_graph_with_diffbot(documents: List[Document], api_key: str) -> DiffbotGraphTransformer:\n",
    "    \"\"\"Extract graph data from documents using Diffbot.\"\"\"\n",
    "\n",
    "    print(\"Extracting graph data with Diffbot...\")\n",
    "    try:\n",
    "        graph_transformer = DiffbotGraphTransformer(api_key)\n",
    "        graph = graph_transformer.convert_to_graph_documents(documents)\n",
    "        print(\"Graph extraction complete.\")\n",
    "\n",
    "        return graph\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during graph extraction: {e}\")\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dfd5ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting graph data with Diffbot...\n",
      "Graph extraction complete.\n",
      "Extracted graph with 368 nodes and 614 edges.\n"
     ]
    }
   ],
   "source": [
    "# Example usage of graph extraction\n",
    "wiki_graph = extract_graph_with_diffbot(wiki_document,\n",
    "                                        os.environ[\"DIFFBOT_API_KEY\"])\n",
    "\n",
    "if wiki_graph:\n",
    "    print(f\"Extracted graph with {sum(len(page.nodes) for page in wiki_graph)} nodes and \\\n",
    "{sum(len(page.relationships) for page in wiki_graph)} edges.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5adee16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample nodes and relationships:\n",
      "Node: id='http://www.wikidata.org/entity/Q3571662' type='Person' properties={'name': 'Yann LeCun', 'positionHeld': 'computer scientist', 'dateOfBirth': '1960-07-08'}\n",
      "Node: id='Adaptive Systems Research Department' type='Organization' properties={'name': 'Adaptive Systems Research Department'}\n",
      "Node: id='http://www.wikidata.org/entity/Q217365' type='Organization' properties={'name': 'Bell Laboratories'}\n",
      "Relationship: source=Node(id='http://www.wikidata.org/entity/Q3571662', type='Person', properties={}) target=Node(id='Adaptive Systems Research Department', type='Organization', properties={}) type='EMPLOYEE_OR_MEMBER_OF' properties={'evidence': 'In 1988, LeCun joined the Adaptive Systems Research Department at AT&T Bell Laboratories in Holmdel, New Jersey, United States, headed by Lawrence D. Jackel, where he developed a number of new machine learning methods, such as a biologically inspired model of image recognition called convolutional neural networks (LeNet), the\"Optimal Brain Damage\" regularization methods, and the Graph Transformer Networks method (similar to conditional random field), which he applied to handwriting recognition and', 'startTime': '1988'}\n",
      "Relationship: source=Node(id='http://www.wikidata.org/entity/Q3571662', type='Person', properties={}) target=Node(id='http://www.wikidata.org/entity/Q217365', type='Organization', properties={}) type='EMPLOYEE_OR_MEMBER_OF' properties={'evidence': 'In 1988, LeCun joined the Adaptive Systems Research Department at AT&T Bell Laboratories in Holmdel, New Jersey, United States, headed by Lawrence D. Jackel, where he developed a number of new machine learning methods, such as a biologically inspired model of image recognition called convolutional neural networks (LeNet), the\"Optimal Brain Damage\" regularization methods, and the Graph Transformer Networks method (similar to conditional random field), which he applied to handwriting recognition and'}\n",
      "Relationship: source=Node(id='http://www.wikidata.org/entity/Q3571662', type='Person', properties={}) target=Node(id='Lawrence D. Jackel', type='Person', properties={}) type='WORK_RELATIONSHIP' properties={'evidence': 'In 1988, LeCun joined the Adaptive Systems Research Department at AT&T Bell Laboratories in Holmdel, New Jersey, United States, headed by Lawrence D. Jackel, where he developed a number of new machine learning methods, such as a biologically inspired model of image recognition called convolutional neural networks (LeNet), the\"Optimal Brain Damage\" regularization methods, and the Graph Transformer Networks method (similar to conditional random field), which he applied to handwriting recognition and'}\n"
     ]
    }
   ],
   "source": [
    "if wiki_graph:\n",
    "    print(\"Sample nodes and relationships:\")\n",
    "    # Display first 3 nodes and relationships from the first page\n",
    "    page = wiki_graph[0]\n",
    "    if page:\n",
    "        for node in page.nodes[:3]:       \n",
    "            print(f\"Node: {node}\")\n",
    "        for relationship in page.relationships[:3]:\n",
    "            print(f\"Relationship: {relationship}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f17f321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APOC, GDS and Bloom will be auto-installed by Neo4j.\n",
      "Neo4j Community launched with APOC + GDS + Bloom! Awaiting startup...\n",
      "Neo4j ready at bolt://localhost:7687 (default DB: db)\n"
     ]
    }
   ],
   "source": [
    "def setup_neo4j_with_plugins():\n",
    "    container_name = \"neo4j\"\n",
    "    password = \"password\"\n",
    "\n",
    "    # Create local files\n",
    "    os.makedirs(\"./neo4j_data/plugins\", exist_ok=True)\n",
    "    os.makedirs(\"./neo4j_data/data\", exist_ok=True)\n",
    "    os.makedirs(\"./neo4j_data/logs\", exist_ok=True)\n",
    "\n",
    "    # Downloading the plugins (APOC, GDS, Bloom)\n",
    "    print(\"APOC, GDS and Bloom will be auto-installed by Neo4j.\")\n",
    "\n",
    "    # Stop and delete the old container if it exists\n",
    "    subprocess.run([\"docker\", \"stop\", container_name], stderr=subprocess.DEVNULL)\n",
    "    subprocess.run([\"docker\", \"rm\", container_name], stderr=subprocess.DEVNULL)\n",
    "\n",
    "    # Absolute paths for Docker\n",
    "    paths = {p: os.path.abspath(f\"./neo4j_data/{p}\") \n",
    "             for p in [\"plugins\", \"data\", \"logs\"]}\n",
    "\n",
    "    # Launch of Neo4j Community with Docker\n",
    "    cmd = [\n",
    "        \"docker\", \"run\", \"--name\", container_name,\n",
    "        \"-p\", \"7474:7474\", \"-p\", \"7687:7687\",\n",
    "        \"-e\", f\"NEO4J_AUTH=neo4j/{password}\",                               # Authentication\n",
    "        \"-e\", 'NEO4J_PLUGINS=[\"apoc\",\"graph-data-science\",\"bloom\"]',        # Install plugins automatically\n",
    "        \"-e\", \"NEO4J_dbms_security_procedures_unrestricted=apoc.*,gds.*\",   # Allow APOC & GDS procedures\n",
    "        \"-e\", \"NEO4J_dbms_security_procedures_allowlist=apoc.*,gds.*\",\n",
    "        \"-e\", \"NEO4J_server_config_strict__validation_enabled=false\",       # Disable strict validation to prevent APOC errors\n",
    "        \"-v\", f\"{paths['plugins']}:/plugins\",                               # Mount volumes\n",
    "        \"-v\", f\"{paths['data']}:/data\",\n",
    "        \"-v\", f\"{paths['logs']}:/logs\",\n",
    "        \"-d\", \"neo4j:5.20\"                                                  # Use Neo4j 5.20 Community Edition\n",
    "    ]\n",
    "\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n",
    "    print(\"Neo4j Community launched with APOC + GDS + Bloom! Awaiting startup...\")\n",
    "    time.sleep(20)\n",
    "    print(\"Neo4j ready at bolt://localhost:7687 (default DB: db)\")\n",
    "\n",
    "\n",
    "# Run setup\n",
    "setup_neo4j_with_plugins()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4da5041f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE        COMMAND                  CREATED        STATUS              PORTS                                                                                      NAMES\n",
      "86dccdae88fd   neo4j:5.20   \"tini -g -- /startupâ€¦\"   13 hours ago   Up About a minute   0.0.0.0:7474->7474/tcp, [::]:7474->7474/tcp, 0.0.0.0:7687->7687/tcp, [::]:7687->7687/tcp   neo4j\n"
     ]
    }
   ],
   "source": [
    "# Displays running Docker containers\n",
    "!docker ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeadaf0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<neo4j._sync.driver.BoltDriver at 0x1cd48b37ed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "URI = \"bolt://localhost:7687\"\n",
    "AUTH = (\"neo4j\", \"password\")\n",
    "\n",
    "# Connect to Neo4j\n",
    "try:\n",
    "    driver = GraphDatabase.driver(URI, auth=AUTH)\n",
    "    driver.verify_connectivity()  # raises if cannot connect\n",
    "except Exception as e:\n",
    "    print(\"Neo4j connection failed:\", e)\n",
    "    raise\n",
    "\n",
    "# print the driver object \n",
    "display(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13effcad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph documents added to Neo4j.\n"
     ]
    }
   ],
   "source": [
    "# Connect to Neo4j using langchain_neo4j\n",
    "graph = Neo4jGraph(\n",
    "    url=URI,\n",
    "    username=AUTH[0],\n",
    "    password=AUTH[1],\n",
    "    database=\"shop\",\n",
    "    refresh_schema=False)\n",
    "\n",
    "# --- IGNORE ---\n",
    "#graph.query(\"MATCH (n)-[r]->(m) DELETE r\")\n",
    "#graph.query(\"MATCH (n) DELETE n\")\n",
    "\n",
    "graph.add_graph_documents(wiki_graph)\n",
    "print(\"Graph documents added to Neo4j.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
